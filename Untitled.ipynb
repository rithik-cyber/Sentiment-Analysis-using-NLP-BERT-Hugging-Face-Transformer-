{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cae6ff8-3cf7-42e6-ac25-61c3530bac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e74afe7-639f-4e5e-b01e-9e8cd5a4a967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netclan20241017</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-ml-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netclan20241018</td>\n",
       "      <td>https://insights.blackcoffer.com/enhancing-fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netclan20241019</td>\n",
       "      <td>https://insights.blackcoffer.com/roas-dashboar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netclan20241020</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netclan20241021</td>\n",
       "      <td>https://insights.blackcoffer.com/development-o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL\n",
       "0  Netclan20241017  https://insights.blackcoffer.com/ai-and-ml-bas...\n",
       "1  Netclan20241018  https://insights.blackcoffer.com/enhancing-fro...\n",
       "2  Netclan20241019  https://insights.blackcoffer.com/roas-dashboar...\n",
       "3  Netclan20241020  https://insights.blackcoffer.com/efficient-pro...\n",
       "4  Netclan20241021  https://insights.blackcoffer.com/development-o..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Excel file\n",
    "input_file = \"Input.xlsx\"  # make sure this file is in the same folder as your notebook\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Preview the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b05efa73-dca9-495d-8cf7-1c49b8369a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Complexity to Clarity: Transforming Data into Decisions through Mixed Modelling AWS CodePipeline is utilized for automatically building and deploying Lambda functions in AWS Dockerize the AWS Lambda for serverless architecture Infrastructure Automation AI audio and text conversational bot using livekit AI Receptionist | Voice Call Center | AI Lawyer | AI Sales Representative | AI Representative | AI Doctor | AI Coach | AI... Face Recognition with Deepfills Framework – Deepface Development of EA Robot for Automated Trading The Ultimate Collection of Multimedia Tools for Video Editing & Screen Recording (2024 Edition) Rising IT cities and its impact on the economy, environment, infrastructure, and city life by the year 2040. Rising IT Cities and Their Impact on the Economy, Environment, Infrastructure, and City Life in Future Internet Demand’s Evolution, Communication Impact, and 2035’s Alternative Pathways AI tools for mechanical engineering, categorized based on their applications\n"
     ]
    }
   ],
   "source": [
    "# Take first URL\n",
    "test_url = df.iloc[0]['URL']  # change 'URL' to the actual column name from your Excel\n",
    "\n",
    "# Send request\n",
    "response = requests.get(test_url)\n",
    "\n",
    "# Parse HTML\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract all text from <p> tags\n",
    "paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "article_text = \" \".join(paragraphs)\n",
    "\n",
    "# Print a preview\n",
    "print(article_text[:1000])  # first 1000 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e353ace-eb83-44be-a19e-9796673ce8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: ScrapedArticles/article_1.txt\n",
      "✅ Saved: ScrapedArticles/article_2.txt\n",
      "✅ Saved: ScrapedArticles/article_3.txt\n",
      "✅ Saved: ScrapedArticles/article_4.txt\n",
      "✅ Saved: ScrapedArticles/article_5.txt\n",
      "✅ Saved: ScrapedArticles/article_6.txt\n",
      "✅ Saved: ScrapedArticles/article_7.txt\n",
      "✅ Saved: ScrapedArticles/article_8.txt\n",
      "✅ Saved: ScrapedArticles/article_9.txt\n",
      "✅ Saved: ScrapedArticles/article_10.txt\n",
      "✅ Saved: ScrapedArticles/article_11.txt\n",
      "✅ Saved: ScrapedArticles/article_12.txt\n",
      "✅ Saved: ScrapedArticles/article_13.txt\n",
      "✅ Saved: ScrapedArticles/article_14.txt\n",
      "✅ Saved: ScrapedArticles/article_15.txt\n",
      "✅ Saved: ScrapedArticles/article_16.txt\n",
      "✅ Saved: ScrapedArticles/article_17.txt\n",
      "✅ Saved: ScrapedArticles/article_18.txt\n",
      "✅ Saved: ScrapedArticles/article_19.txt\n",
      "✅ Saved: ScrapedArticles/article_20.txt\n",
      "✅ Saved: ScrapedArticles/article_21.txt\n",
      "✅ Saved: ScrapedArticles/article_22.txt\n",
      "✅ Saved: ScrapedArticles/article_23.txt\n",
      "✅ Saved: ScrapedArticles/article_24.txt\n",
      "✅ Saved: ScrapedArticles/article_25.txt\n",
      "✅ Saved: ScrapedArticles/article_26.txt\n",
      "✅ Saved: ScrapedArticles/article_27.txt\n",
      "✅ Saved: ScrapedArticles/article_28.txt\n",
      "✅ Saved: ScrapedArticles/article_29.txt\n",
      "✅ Saved: ScrapedArticles/article_30.txt\n",
      "✅ Saved: ScrapedArticles/article_31.txt\n",
      "✅ Saved: ScrapedArticles/article_32.txt\n",
      "✅ Saved: ScrapedArticles/article_33.txt\n",
      "✅ Saved: ScrapedArticles/article_34.txt\n",
      "✅ Saved: ScrapedArticles/article_35.txt\n",
      "✅ Saved: ScrapedArticles/article_36.txt\n",
      "✅ Saved: ScrapedArticles/article_37.txt\n",
      "✅ Saved: ScrapedArticles/article_38.txt\n",
      "✅ Saved: ScrapedArticles/article_39.txt\n",
      "✅ Saved: ScrapedArticles/article_40.txt\n",
      "✅ Saved: ScrapedArticles/article_41.txt\n",
      "✅ Saved: ScrapedArticles/article_42.txt\n",
      "✅ Saved: ScrapedArticles/article_43.txt\n",
      "✅ Saved: ScrapedArticles/article_44.txt\n",
      "✅ Saved: ScrapedArticles/article_45.txt\n",
      "✅ Saved: ScrapedArticles/article_46.txt\n",
      "✅ Saved: ScrapedArticles/article_47.txt\n",
      "✅ Saved: ScrapedArticles/article_48.txt\n",
      "✅ Saved: ScrapedArticles/article_49.txt\n",
      "✅ Saved: ScrapedArticles/article_50.txt\n",
      "✅ Saved: ScrapedArticles/article_51.txt\n",
      "✅ Saved: ScrapedArticles/article_52.txt\n",
      "✅ Saved: ScrapedArticles/article_53.txt\n",
      "✅ Saved: ScrapedArticles/article_54.txt\n",
      "✅ Saved: ScrapedArticles/article_55.txt\n",
      "✅ Saved: ScrapedArticles/article_56.txt\n",
      "✅ Saved: ScrapedArticles/article_57.txt\n",
      "✅ Saved: ScrapedArticles/article_58.txt\n",
      "✅ Saved: ScrapedArticles/article_59.txt\n",
      "✅ Saved: ScrapedArticles/article_60.txt\n",
      "✅ Saved: ScrapedArticles/article_61.txt\n",
      "✅ Saved: ScrapedArticles/article_62.txt\n",
      "✅ Saved: ScrapedArticles/article_63.txt\n",
      "✅ Saved: ScrapedArticles/article_64.txt\n",
      "✅ Saved: ScrapedArticles/article_65.txt\n",
      "✅ Saved: ScrapedArticles/article_66.txt\n",
      "✅ Saved: ScrapedArticles/article_67.txt\n",
      "✅ Saved: ScrapedArticles/article_68.txt\n",
      "✅ Saved: ScrapedArticles/article_69.txt\n",
      "✅ Saved: ScrapedArticles/article_70.txt\n",
      "✅ Saved: ScrapedArticles/article_71.txt\n",
      "✅ Saved: ScrapedArticles/article_72.txt\n",
      "✅ Saved: ScrapedArticles/article_73.txt\n",
      "✅ Saved: ScrapedArticles/article_74.txt\n",
      "✅ Saved: ScrapedArticles/article_75.txt\n",
      "✅ Saved: ScrapedArticles/article_76.txt\n",
      "✅ Saved: ScrapedArticles/article_77.txt\n",
      "✅ Saved: ScrapedArticles/article_78.txt\n",
      "✅ Saved: ScrapedArticles/article_79.txt\n",
      "✅ Saved: ScrapedArticles/article_80.txt\n",
      "✅ Saved: ScrapedArticles/article_81.txt\n",
      "✅ Saved: ScrapedArticles/article_82.txt\n",
      "✅ Saved: ScrapedArticles/article_83.txt\n",
      "✅ Saved: ScrapedArticles/article_84.txt\n",
      "✅ Saved: ScrapedArticles/article_85.txt\n",
      "✅ Saved: ScrapedArticles/article_86.txt\n",
      "✅ Saved: ScrapedArticles/article_87.txt\n",
      "✅ Saved: ScrapedArticles/article_88.txt\n",
      "✅ Saved: ScrapedArticles/article_89.txt\n",
      "✅ Saved: ScrapedArticles/article_90.txt\n",
      "✅ Saved: ScrapedArticles/article_91.txt\n",
      "✅ Saved: ScrapedArticles/article_92.txt\n",
      "✅ Saved: ScrapedArticles/article_93.txt\n",
      "✅ Saved: ScrapedArticles/article_94.txt\n",
      "✅ Saved: ScrapedArticles/article_95.txt\n",
      "✅ Saved: ScrapedArticles/article_96.txt\n",
      "✅ Saved: ScrapedArticles/article_97.txt\n",
      "✅ Saved: ScrapedArticles/article_98.txt\n",
      "✅ Saved: ScrapedArticles/article_99.txt\n",
      "✅ Saved: ScrapedArticles/article_100.txt\n",
      "✅ Saved: ScrapedArticles/article_101.txt\n",
      "✅ Saved: ScrapedArticles/article_102.txt\n",
      "✅ Saved: ScrapedArticles/article_103.txt\n",
      "✅ Saved: ScrapedArticles/article_104.txt\n",
      "✅ Saved: ScrapedArticles/article_105.txt\n",
      "✅ Saved: ScrapedArticles/article_106.txt\n",
      "✅ Saved: ScrapedArticles/article_107.txt\n",
      "✅ Saved: ScrapedArticles/article_108.txt\n",
      "✅ Saved: ScrapedArticles/article_109.txt\n",
      "✅ Saved: ScrapedArticles/article_110.txt\n",
      "✅ Saved: ScrapedArticles/article_111.txt\n",
      "✅ Saved: ScrapedArticles/article_112.txt\n",
      "✅ Saved: ScrapedArticles/article_113.txt\n",
      "✅ Saved: ScrapedArticles/article_114.txt\n",
      "✅ Saved: ScrapedArticles/article_115.txt\n",
      "✅ Saved: ScrapedArticles/article_116.txt\n",
      "✅ Saved: ScrapedArticles/article_117.txt\n",
      "✅ Saved: ScrapedArticles/article_118.txt\n",
      "✅ Saved: ScrapedArticles/article_119.txt\n",
      "✅ Saved: ScrapedArticles/article_120.txt\n",
      "✅ Saved: ScrapedArticles/article_121.txt\n",
      "✅ Saved: ScrapedArticles/article_122.txt\n",
      "✅ Saved: ScrapedArticles/article_123.txt\n",
      "✅ Saved: ScrapedArticles/article_124.txt\n",
      "✅ Saved: ScrapedArticles/article_125.txt\n",
      "✅ Saved: ScrapedArticles/article_126.txt\n",
      "✅ Saved: ScrapedArticles/article_127.txt\n",
      "✅ Saved: ScrapedArticles/article_128.txt\n",
      "✅ Saved: ScrapedArticles/article_129.txt\n",
      "✅ Saved: ScrapedArticles/article_130.txt\n",
      "✅ Saved: ScrapedArticles/article_131.txt\n",
      "✅ Saved: ScrapedArticles/article_132.txt\n",
      "✅ Saved: ScrapedArticles/article_133.txt\n",
      "✅ Saved: ScrapedArticles/article_134.txt\n",
      "✅ Saved: ScrapedArticles/article_135.txt\n",
      "✅ Saved: ScrapedArticles/article_136.txt\n",
      "✅ Saved: ScrapedArticles/article_137.txt\n",
      "✅ Saved: ScrapedArticles/article_138.txt\n",
      "✅ Saved: ScrapedArticles/article_139.txt\n",
      "✅ Saved: ScrapedArticles/article_140.txt\n",
      "✅ Saved: ScrapedArticles/article_141.txt\n",
      "✅ Saved: ScrapedArticles/article_142.txt\n",
      "✅ Saved: ScrapedArticles/article_143.txt\n",
      "✅ Saved: ScrapedArticles/article_144.txt\n",
      "✅ Saved: ScrapedArticles/article_145.txt\n",
      "✅ Saved: ScrapedArticles/article_146.txt\n",
      "✅ Saved: ScrapedArticles/article_147.txt\n"
     ]
    }
   ],
   "source": [
    "# Create a folder for scraped articles\n",
    "os.makedirs(\"ScrapedArticles\", exist_ok=True)\n",
    "\n",
    "# Change this to match the actual column name in your Excel file\n",
    "url_column = \"URL\"\n",
    "\n",
    "# Loop through each row\n",
    "for idx, row in df.iterrows():\n",
    "    url = row[url_column]\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract text from <p> tags\n",
    "        paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "        article_text = \" \".join(paragraphs)\n",
    "\n",
    "        # Save to text file\n",
    "        file_name = f\"ScrapedArticles/article_{idx+1}.txt\"\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(article_text)\n",
    "\n",
    "        print(f\"✅ Saved: {file_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error scraping {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74c0a1a6-6200-4531-a3a0-b41ce02960dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: C:\\Users\\DELL\\OneDrive\\Desktop\\DataNLP\n",
      "Scraped folder exists: True\n",
      "Input file exists: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - imports & config\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Where scraped text files are:\n",
    "SCRAPED_DIR = Path(\"ScrapedArticles\")\n",
    "\n",
    "# Input excel\n",
    "INPUT_XL = Path(\"Input.xlsx\")\n",
    "\n",
    "# Output file\n",
    "OUTPUT_XL = Path(\"Final_Output.xlsx\")\n",
    "\n",
    "# Names of the columns in Input.xlsx (change if needed)\n",
    "url_id_col = \"URL_ID\"\n",
    "url_col = \"URL\"\n",
    "\n",
    "# Lexicon folder names to try (you said 'Lexicon' exists)\n",
    "LEXICON_DIR_CANDIDATES = [Path(\"Lexicon\"), Path(\"lexicons\"), Path(\"Lexicon/lexicons\"), Path(\"lexicons/lexicon\")]\n",
    "\n",
    "print(\"Working directory:\", Path.cwd())\n",
    "print(\"Scraped folder exists:\", SCRAPED_DIR.exists())\n",
    "print(\"Input file exists:\", INPUT_XL.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79ff0aa6-c437-4288-94c8-d078d1448012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded lexicons from Lexicon\n",
      "Counts: pos= 1997 neg= 4760 stopwords= 12761\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 - load lexicons (if present); fallback small sets if not found\n",
    "def load_wordlist(path):\n",
    "    s = set()\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            token = re.sub(r\"[^A-Za-z]\", \"\", line).strip().lower()\n",
    "            if token:\n",
    "                s.add(token)\n",
    "    return s\n",
    "\n",
    "# try candidate folders\n",
    "lexdir = None\n",
    "for c in LEXICON_DIR_CANDIDATES:\n",
    "    if c.exists():\n",
    "        lexdir = c\n",
    "        break\n",
    "\n",
    "if lexdir:\n",
    "    pos_path = lexdir / \"positive-words.txt\"\n",
    "    neg_path = lexdir / \"negative-words.txt\"\n",
    "    stop_path = lexdir / \"stopwords.txt\"\n",
    "    pos_words = load_wordlist(pos_path) if pos_path.exists() else set()\n",
    "    neg_words = load_wordlist(neg_path) if neg_path.exists() else set()\n",
    "    stopwords = load_wordlist(stop_path) if stop_path.exists() else set()\n",
    "    print(\"Loaded lexicons from\", lexdir)\n",
    "else:\n",
    "    pos_words = {\"good\",\"great\",\"excellent\",\"positive\",\"fortunate\",\"success\",\"improve\",\"growth\",\"benefit\"}\n",
    "    neg_words = {\"bad\",\"poor\",\"negative\",\"loss\",\"decline\",\"risk\",\"fail\",\"problem\",\"worse\"}\n",
    "    stopwords = {\"a\",\"an\",\"the\",\"in\",\"on\",\"and\",\"is\",\"are\",\"this\",\"that\",\"it\",\"to\",\"of\",\"for\",\"with\",\"by\",\"as\",\"from\"}\n",
    "    print(\"Lexicon folder not found. Using fallback small lexicons.\")\n",
    "\n",
    "# Ensure personal pronoun tokens are not removed if you later remove stopwords; but for counting we remove stopwords.\n",
    "for w in [\"i\",\"we\",\"my\",\"ours\",\"us\"]:\n",
    "    if w not in stopwords:\n",
    "        stopwords.add(w)   # NOTE: earlier scripts included these in stopwords; we keep it consistent\n",
    "print(\"Counts: pos=\", len(pos_words), \"neg=\", len(neg_words), \"stopwords=\", len(stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f02d8728-025a-4f7e-b901-192fcfcfcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - tokenizers, syllable counter, and main analysis function\n",
    "WORD_RE = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?\")\n",
    "SENT_SPLIT_RE = re.compile(r\"[.!?]+\")\n",
    "PPRON_RE = re.compile(r\"\\b(I|we|my|ours|us)\\b\", re.IGNORECASE)\n",
    "VOWELS = \"aeiouy\"\n",
    "\n",
    "def tokenize_words(text):\n",
    "    return WORD_RE.findall(text)\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    return [s.strip() for s in SENT_SPLIT_RE.split(text) if s.strip()]\n",
    "\n",
    "def count_syllables(word):\n",
    "    w = re.sub(r\"[^a-z]\", \"\", word.lower())\n",
    "    if not w:\n",
    "        return 0\n",
    "    syllables = 0\n",
    "    prev_is_vowel = False\n",
    "    for ch in w:\n",
    "        is_vowel = ch in VOWELS\n",
    "        if is_vowel and not prev_is_vowel:\n",
    "            syllables += 1\n",
    "        prev_is_vowel = is_vowel\n",
    "    if w.endswith(\"e\") and syllables > 1:\n",
    "        syllables -= 1\n",
    "    return max(1, syllables)\n",
    "\n",
    "def analyze_text(text, pos_lex, neg_lex, stopwords):\n",
    "    sentences = tokenize_sentences(text)\n",
    "    words_all = [w for w in tokenize_words(text)]\n",
    "    # Lowercase for matching\n",
    "    words_lower = [w.lower() for w in words_all]\n",
    "    # Remove stopwords for counts (as assignment typically expects)\n",
    "    words_clean = [w for w in words_lower if w not in stopwords]\n",
    "\n",
    "    pos_score = sum(1 for w in words_clean if w in pos_lex)\n",
    "    neg_score = sum(1 for w in words_clean if w in neg_lex)\n",
    "\n",
    "    polarity = (pos_score - neg_score) / ((pos_score + neg_score) + 1e-6)\n",
    "    subjectivity = (pos_score + neg_score) / ((len(words_clean)) + 1e-6)\n",
    "\n",
    "    syllable_counts = [count_syllables(w) for w in words_clean]\n",
    "    complex_flags = [1 if s > 2 else 0 for s in syllable_counts]\n",
    "    complex_count = sum(complex_flags)\n",
    "\n",
    "    words_per_sentence = [len(tokenize_words(s)) for s in sentences] if sentences else [0]\n",
    "    avg_sentence_length = sum(words_per_sentence) / len(words_per_sentence) if words_per_sentence else 0.0\n",
    "    avg_words_per_sentence = avg_sentence_length\n",
    "\n",
    "    total_words = len(words_clean)\n",
    "    pct_complex = (complex_count / total_words) if total_words else 0.0\n",
    "    fog_index = 0.4 * (avg_sentence_length + 100 * pct_complex)\n",
    "    syllables_per_word = (sum(syllable_counts) / total_words) if total_words else 0.0\n",
    "\n",
    "    # Personal pronouns: search in original text to preserve case-sensitivity exclusion for \"US\"\n",
    "    personal_pronouns = 0\n",
    "    for m in PPRON_RE.finditer(text):\n",
    "        tok = m.group(0)\n",
    "        if tok == \"US\":\n",
    "            continue\n",
    "        personal_pronouns += 1\n",
    "\n",
    "    total_chars = sum(len(re.sub(r\"[^A-Za-z]\", \"\", w)) for w in words_clean)\n",
    "    avg_word_length = (total_chars / total_words) if total_words else 0.0\n",
    "\n",
    "    return {\n",
    "        \"POSITIVE SCORE\": pos_score,\n",
    "        \"NEGATIVE SCORE\": neg_score,\n",
    "        \"POLARITY SCORE\": polarity,\n",
    "        \"SUBJECTIVITY SCORE\": subjectivity,\n",
    "        \"AVG SENTENCE LENGTH\": avg_sentence_length,\n",
    "        \"PERCENTAGE OF COMPLEX WORDS\": pct_complex,\n",
    "        \"FOG INDEX\": fog_index,\n",
    "        \"AVG NUMBER OF WORDS PER SENTENCE\": avg_words_per_sentence,\n",
    "        \"COMPLEX WORD COUNT\": complex_count,\n",
    "        \"WORD COUNT\": total_words,\n",
    "        \"SYLLABLE PER WORD\": syllables_per_word,\n",
    "        \"PERSONAL PRONOUNS\": personal_pronouns,\n",
    "        \"AVG WORD LENGTH\": avg_word_length\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f4056c9-0da8-45de-81ea-12f0c598bcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in Input.xlsx: 147\n",
      "Found 147 / 147 files linked to Input rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'URL_ID': 'Netclan20241017',\n",
       "  'URL': 'https://insights.blackcoffer.com/ai-and-ml-based-youtube-analytics-and-content-creation-tool-for-optimizing-subscriber-engagement-and-content-strategy/',\n",
       "  'file': WindowsPath('ScrapedArticles/article_1.txt')},\n",
       " {'URL_ID': 'Netclan20241018',\n",
       "  'URL': 'https://insights.blackcoffer.com/enhancing-front-end-features-and-functionality-for-improved-user-experience-and-dashboard-accuracy-in-partner-hospital-application/',\n",
       "  'file': WindowsPath('ScrapedArticles/article_2.txt')},\n",
       " {'URL_ID': 'Netclan20241019',\n",
       "  'URL': 'https://insights.blackcoffer.com/roas-dashboard-for-campaign-wise-google-ads-budget-tracking-using-google-ads-ap/',\n",
       "  'file': WindowsPath('ScrapedArticles/article_3.txt')},\n",
       " {'URL_ID': 'Netclan20241020',\n",
       "  'URL': 'https://insights.blackcoffer.com/efficient-processing-and-analysis-of-financial-data-from-pdf-files-addressing-formatting-inconsistencies-and-ensuring-data-integrity-for-a-toyota-dealership-management-firm/',\n",
       "  'file': WindowsPath('ScrapedArticles/article_4.txt')},\n",
       " {'URL_ID': 'Netclan20241021',\n",
       "  'URL': 'https://insights.blackcoffer.com/development-of-ea-robot-for-automated-trading/',\n",
       "  'file': WindowsPath('ScrapedArticles/article_5.txt')},\n",
       " {'URL_ID': 'Netclan20241022',\n",
       "  'URL': 'https://insights.blackcoffer.com/ai-and-ml-based-youtube-analytics-and-content-creation-tool-for-optimizing-subscriber-engagement-and-content-strategy/',\n",
       "  'file': WindowsPath('ScrapedArticles/article_6.txt')},\n",
       " {'URL_ID': 'Netclan20241023',\n",
       "  'URL': 'https://insights.blackcoffer.com/enhancing-front-end-features-and-functionality-for-improved-user-experience-and-dashboard-accuracy-in-partner-hospital-application/',\n",
       "  'file': WindowsPath('ScrapedArticles/article_7.txt')},\n",
       " {'URL_ID': 'Netclan20241024',\n",
       "  'URL': 'https://insights.blackcoffer.com/roas-dashboard-for-campaign-wise-google-ads-budget-tracking-using-google-ads-ap/',\n",
       "  'file': WindowsPath('ScrapedArticles/article_8.txt')},\n",
       " {'URL_ID': 'Netclan20241025',\n",
       "  'URL': 'https://insights.blackcoffer.com/efficient-processing-and-analysis-of-financial-data-from-pdf-files-addressing-formatting-inconsistencies-and-ensuring-data-integrity-for-a-toyota-dealership-management-firm/',\n",
       "  'file': WindowsPath('ScrapedArticles/article_9.txt')},\n",
       " {'URL_ID': 'Netclan20241026',\n",
       "  'URL': 'https://insights.blackcoffer.com/transforming-and-managing-a-large-scale-sql-pedigree-database-to-neo4j-graph-db/',\n",
       "  'file': WindowsPath('ScrapedArticles/article_10.txt')}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4 - create a mapping from each Input.xlsx row to the scraped text file\n",
    "df_in = pd.read_excel(INPUT_XL)\n",
    "print(\"Rows in Input.xlsx:\", len(df_in))\n",
    "\n",
    "file_map = []   # list of dicts {URL_ID, URL, file_path or None}\n",
    "for idx, row in df_in.iterrows():\n",
    "    url_id = str(row[url_id_col]).strip()\n",
    "    url = str(row[url_col]).strip()\n",
    "    # candidate filenames\n",
    "    candidates = [\n",
    "        SCRAPED_DIR / f\"{url_id}.txt\",\n",
    "        SCRAPED_DIR / f\"article_{idx+1}.txt\",\n",
    "        SCRAPED_DIR / f\"{idx+1}.txt\",\n",
    "        SCRAPED_DIR / f\"article_{url_id}.txt\",\n",
    "    ]\n",
    "    found = None\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            found = c\n",
    "            break\n",
    "    # last resort: find any file that contains the url_id string\n",
    "    if not found:\n",
    "        for p in SCRAPED_DIR.iterdir():\n",
    "            if p.is_file() and url_id in p.name:\n",
    "                found = p\n",
    "                break\n",
    "    file_map.append({\"URL_ID\": url_id, \"URL\": url, \"file\": found})\n",
    "\n",
    "# summary\n",
    "found_count = sum(1 for r in file_map if r[\"file\"] is not None)\n",
    "print(f\"Found {found_count} / {len(file_map)} files linked to Input rows\")\n",
    "# show first 10 mappings\n",
    "file_map[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "771c13f3-aacd-4ca7-a223-368d1a6f3014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netclan20241017</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-ml-bas...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>22.294118</td>\n",
       "      <td>0.470339</td>\n",
       "      <td>27.731206</td>\n",
       "      <td>22.294118</td>\n",
       "      <td>111</td>\n",
       "      <td>236</td>\n",
       "      <td>2.576271</td>\n",
       "      <td>3</td>\n",
       "      <td>7.783898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netclan20241018</td>\n",
       "      <td>https://insights.blackcoffer.com/enhancing-fro...</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>22.361702</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>24.224681</td>\n",
       "      <td>22.361702</td>\n",
       "      <td>191</td>\n",
       "      <td>500</td>\n",
       "      <td>2.380000</td>\n",
       "      <td>9</td>\n",
       "      <td>7.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netclan20241019</td>\n",
       "      <td>https://insights.blackcoffer.com/roas-dashboar...</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.058182</td>\n",
       "      <td>22.578947</td>\n",
       "      <td>0.407273</td>\n",
       "      <td>25.322488</td>\n",
       "      <td>22.578947</td>\n",
       "      <td>112</td>\n",
       "      <td>275</td>\n",
       "      <td>2.440000</td>\n",
       "      <td>3</td>\n",
       "      <td>7.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netclan20241020</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-pro...</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.076289</td>\n",
       "      <td>19.513514</td>\n",
       "      <td>0.556701</td>\n",
       "      <td>30.073447</td>\n",
       "      <td>19.513514</td>\n",
       "      <td>270</td>\n",
       "      <td>485</td>\n",
       "      <td>2.736082</td>\n",
       "      <td>6</td>\n",
       "      <td>7.950515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netclan20241021</td>\n",
       "      <td>https://insights.blackcoffer.com/development-o...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024793</td>\n",
       "      <td>21.666667</td>\n",
       "      <td>0.458678</td>\n",
       "      <td>27.013774</td>\n",
       "      <td>21.666667</td>\n",
       "      <td>111</td>\n",
       "      <td>242</td>\n",
       "      <td>2.566116</td>\n",
       "      <td>3</td>\n",
       "      <td>7.541322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL  \\\n",
       "0  Netclan20241017  https://insights.blackcoffer.com/ai-and-ml-bas...   \n",
       "1  Netclan20241018  https://insights.blackcoffer.com/enhancing-fro...   \n",
       "2  Netclan20241019  https://insights.blackcoffer.com/roas-dashboar...   \n",
       "3  Netclan20241020  https://insights.blackcoffer.com/efficient-pro...   \n",
       "4  Netclan20241021  https://insights.blackcoffer.com/development-o...   \n",
       "\n",
       "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0               9               0        1.000000            0.038136   \n",
       "1              15               7        0.363636            0.044000   \n",
       "2              14               2        0.750000            0.058182   \n",
       "3              26              11        0.405405            0.076289   \n",
       "4               6               0        1.000000            0.024793   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0            22.294118                     0.470339  27.731206   \n",
       "1            22.361702                     0.382000  24.224681   \n",
       "2            22.578947                     0.407273  25.322488   \n",
       "3            19.513514                     0.556701  30.073447   \n",
       "4            21.666667                     0.458678  27.013774   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                         22.294118                 111         236   \n",
       "1                         22.361702                 191         500   \n",
       "2                         22.578947                 112         275   \n",
       "3                         19.513514                 270         485   \n",
       "4                         21.666667                 111         242   \n",
       "\n",
       "   SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0           2.576271                  3         7.783898  \n",
       "1           2.380000                  9         7.164000  \n",
       "2           2.440000                  3         7.454545  \n",
       "3           2.736082                  6         7.950515  \n",
       "4           2.566116                  3         7.541322  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5 - analyze all found files and build final DataFrame in the required column order\n",
    "results = []\n",
    "for rec in file_map:\n",
    "    uid = rec[\"URL_ID\"]\n",
    "    url = rec[\"URL\"]\n",
    "    fpath = rec[\"file\"]\n",
    "    if fpath is None:\n",
    "        # if missing, create NaN/zeros\n",
    "        results.append({\n",
    "            \"URL_ID\": uid,\n",
    "            \"URL\": url,\n",
    "            \"POSITIVE SCORE\": None,\n",
    "            \"NEGATIVE SCORE\": None,\n",
    "            \"POLARITY SCORE\": None,\n",
    "            \"SUBJECTIVITY SCORE\": None,\n",
    "            \"AVG SENTENCE LENGTH\": None,\n",
    "            \"PERCENTAGE OF COMPLEX WORDS\": None,\n",
    "            \"FOG INDEX\": None,\n",
    "            \"AVG NUMBER OF WORDS PER SENTENCE\": None,\n",
    "            \"COMPLEX WORD COUNT\": None,\n",
    "            \"WORD COUNT\": None,\n",
    "            \"SYLLABLE PER WORD\": None,\n",
    "            \"PERSONAL PRONOUNS\": None,\n",
    "            \"AVG WORD LENGTH\": None\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    text = fpath.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    # optionally include title extraction if your file format includes title in first line - it's fine as-is\n",
    "    metrics = analyze_text(text, pos_words, neg_words, stopwords)\n",
    "    row = {\"URL_ID\": uid, \"URL\": url}\n",
    "    row.update(metrics)\n",
    "    results.append(row)\n",
    "\n",
    "# final df and reorder columns exactly as required\n",
    "cols = [\n",
    "    \"URL_ID\", \"URL\", \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\n",
    "    \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\",\n",
    "    \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\", \"WORD COUNT\",\n",
    "    \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"\n",
    "]\n",
    "df_out = pd.DataFrame(results, columns=cols)\n",
    "df_out.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b69eb42-19d3-4f08-bd07-d15f7b12a2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Final_Output.xlsx and Final_Output.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 - save output\n",
    "df_out.to_excel(OUTPUT_XL, index=False)\n",
    "df_out.to_csv(str(OUTPUT_XL.with_suffix(\".csv\")), index=False, encoding=\"utf-8\")\n",
    "print(\"Saved:\", OUTPUT_XL, \"and\", OUTPUT_XL.with_suffix(\".csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16353635-99bd-4988-8be9-6e2ff62abd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID_x</th>\n",
       "      <th>URL</th>\n",
       "      <th>URL_ID_y</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netclan20241017</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-ml-bas...</td>\n",
       "      <td>Netclan20241017</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>22.294118</td>\n",
       "      <td>0.470339</td>\n",
       "      <td>27.731206</td>\n",
       "      <td>22.294118</td>\n",
       "      <td>111</td>\n",
       "      <td>236</td>\n",
       "      <td>2.576271</td>\n",
       "      <td>3</td>\n",
       "      <td>7.783898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netclan20241017</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-ml-bas...</td>\n",
       "      <td>Netclan20241022</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>22.294118</td>\n",
       "      <td>0.470339</td>\n",
       "      <td>27.731206</td>\n",
       "      <td>22.294118</td>\n",
       "      <td>111</td>\n",
       "      <td>236</td>\n",
       "      <td>2.576271</td>\n",
       "      <td>3</td>\n",
       "      <td>7.783898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netclan20241018</td>\n",
       "      <td>https://insights.blackcoffer.com/enhancing-fro...</td>\n",
       "      <td>Netclan20241018</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>22.361702</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>24.224681</td>\n",
       "      <td>22.361702</td>\n",
       "      <td>191</td>\n",
       "      <td>500</td>\n",
       "      <td>2.380000</td>\n",
       "      <td>9</td>\n",
       "      <td>7.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netclan20241018</td>\n",
       "      <td>https://insights.blackcoffer.com/enhancing-fro...</td>\n",
       "      <td>Netclan20241023</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>22.361702</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>24.224681</td>\n",
       "      <td>22.361702</td>\n",
       "      <td>191</td>\n",
       "      <td>500</td>\n",
       "      <td>2.380000</td>\n",
       "      <td>9</td>\n",
       "      <td>7.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netclan20241019</td>\n",
       "      <td>https://insights.blackcoffer.com/roas-dashboar...</td>\n",
       "      <td>Netclan20241019</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.058182</td>\n",
       "      <td>22.578947</td>\n",
       "      <td>0.407273</td>\n",
       "      <td>25.322488</td>\n",
       "      <td>22.578947</td>\n",
       "      <td>112</td>\n",
       "      <td>275</td>\n",
       "      <td>2.440000</td>\n",
       "      <td>3</td>\n",
       "      <td>7.454545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          URL_ID_x                                                URL  \\\n",
       "0  Netclan20241017  https://insights.blackcoffer.com/ai-and-ml-bas...   \n",
       "1  Netclan20241017  https://insights.blackcoffer.com/ai-and-ml-bas...   \n",
       "2  Netclan20241018  https://insights.blackcoffer.com/enhancing-fro...   \n",
       "3  Netclan20241018  https://insights.blackcoffer.com/enhancing-fro...   \n",
       "4  Netclan20241019  https://insights.blackcoffer.com/roas-dashboar...   \n",
       "\n",
       "          URL_ID_y  POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  \\\n",
       "0  Netclan20241017               9               0        1.000000   \n",
       "1  Netclan20241022               9               0        1.000000   \n",
       "2  Netclan20241018              15               7        0.363636   \n",
       "3  Netclan20241023              15               7        0.363636   \n",
       "4  Netclan20241019              14               2        0.750000   \n",
       "\n",
       "   SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  \\\n",
       "0            0.038136            22.294118                     0.470339   \n",
       "1            0.038136            22.294118                     0.470339   \n",
       "2            0.044000            22.361702                     0.382000   \n",
       "3            0.044000            22.361702                     0.382000   \n",
       "4            0.058182            22.578947                     0.407273   \n",
       "\n",
       "   FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  \\\n",
       "0  27.731206                         22.294118                 111   \n",
       "1  27.731206                         22.294118                 111   \n",
       "2  24.224681                         22.361702                 191   \n",
       "3  24.224681                         22.361702                 191   \n",
       "4  25.322488                         22.578947                 112   \n",
       "\n",
       "   WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0         236           2.576271                  3         7.783898  \n",
       "1         236           2.576271                  3         7.783898  \n",
       "2         500           2.380000                  9         7.164000  \n",
       "3         500           2.380000                  9         7.164000  \n",
       "4         275           2.440000                  3         7.454545  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_input = pd.read_excel(\"Input.xlsx\")  \n",
    "final_df = pd.merge(df_input, df_out, on=\"URL\", how=\"left\")\n",
    "final_df.to_excel(\"Final_Output.xlsx\", index=False)\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec34c7b4-d5c6-4237-af1f-ab8473e6d7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate column names fixed and saved as Final_Output_cleaned.xlsx\n",
      "Columns now are: ['URL_ID_x', 'URL', 'URL_ID_y', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read Excel file\n",
    "df = pd.read_excel(\"Final_Output.xlsx\")\n",
    "\n",
    "# Make column names unique manually\n",
    "cols = pd.Series(df.columns)\n",
    "for dup in cols[cols.duplicated()].unique():\n",
    "    count = 0\n",
    "    for i in range(len(cols)):\n",
    "        if cols[i] == dup:\n",
    "            count += 1\n",
    "            if count > 1:\n",
    "                cols[i] = f\"{dup}_{count-1}\"  # rename duplicates\n",
    "\n",
    "df.columns = cols\n",
    "\n",
    "# Save cleaned file\n",
    "df.to_excel(\"Final_Output_cleaned.xlsx\", index=False)\n",
    "\n",
    "print(\"Duplicate column names fixed and saved as Final_Output_cleaned.xlsx\")\n",
    "print(\"Columns now are:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a27f099e-15ef-4a26-b8ab-9b27094bb97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          URL_ID_x         URL_ID_y\n",
      "0  Netclan20241017  Netclan20241017\n",
      "1  Netclan20241017  Netclan20241022\n",
      "2  Netclan20241018  Netclan20241018\n",
      "3  Netclan20241018  Netclan20241023\n",
      "4  Netclan20241019  Netclan20241019\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"Final_Output_cleaned.xlsx\")\n",
    "\n",
    "# See first few rows to compare\n",
    "print(df[['URL_ID_x', 'URL_ID_y']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e64dea13-0880-4196-8f46-518fb340873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file saved as Final_Output_ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"Final_Output_cleaned.xlsx\")\n",
    "\n",
    "# Keep URL_ID_x, rename it to URL_ID\n",
    "df = df.drop(columns=['URL_ID_y'])\n",
    "df = df.rename(columns={'URL_ID_x': 'URL_ID'})\n",
    "\n",
    "df.to_excel(\"Final_Output_ready.xlsx\", index=False)\n",
    "print(\"Cleaned file saved as Final_Output_ready.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cd88fdf-8691-4cff-98ef-54b3188493d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed. Cleaned file saved as Final_Output_clean.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(\"Final_Output_ready.xlsx\")\n",
    "\n",
    "# Remove duplicates based on URL_ID and URL\n",
    "df_unique = df.drop_duplicates(subset=['URL_ID', 'URL'], keep='first')\n",
    "\n",
    "# Save the cleaned file\n",
    "df_unique.to_excel(\"Final_Output_clean.xlsx\", index=False)\n",
    "\n",
    "print(\"Duplicates removed. Cleaned file saved as Final_Output_clean.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873045de-f6cd-4bbe-b8c7-d768a0a958d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
